<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Transfer learning in multi-agent RL settings | Rafael Fernandes Cunha</title> <meta name="author" content="Rafael Fernandes Cunha"> <meta name="description" content="Finding and simulating a set of policies to transfer learning in multi-agent reinforcement learning settings"> <meta name="keywords" content="reinforcement learning, multi-agent systems, control theory, machine learning, multi-agent reinforcement learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rafaelcunha2013.github.io/projects/5_tl_sf_ma_pomdp/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Rafael </span>Fernandes Cunha</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Transfer learning in multi-agent RL settings</h1> <p class="post-description">Finding and simulating a set of policies to transfer learning in multi-agent reinforcement learning settings</p> </header> <article> <style>h7:after{content:"\A";white-space:pre}</style> <h6 style="color: #00ab37;display: inline">Course of study:</h6> <h7 style="display: inline;">Artificial Intelligence</h7> <h6 style="color: #00ab37;display: inline">Kind of thesis:</h6> <h7 style="display: inline;">Theoretical analysis and Numerical Simulation</h7> <h6 style="color: #00ab37; display: inline">Programming languages:</h6> <h7 style="display: inline;">Python</h7> <h6 style="color: #00ab37; display: inline">Keywords:</h6> <h7 style="display: inline;">Multi-agent Reinforcement Learning, Transfer Learning, Successor features, Descentralized Partially-Observable Markov decision proccess (DEC-POMDP) </h7> <p><br></p> <h2 style="color: #00ab37;">Problem:</h2> <div class="row"> <div class="col-sm-8 mt-3 mt-md-0" style="text-align: justify;"> Decentralized partially observable Markov decision processes (Dec-POMDPs) provide a general model for decision-making under uncertainty in decentralized settings, but are difficult to solve optimally. Transforming a Dec-POMDP into a continuous-state deterministic MDP with a piecewise-linear and convex value function, and using the fact that planning can be accomplished in a centralized offline manner, while execution can still be decentralized, we can use the occupancy MDP framework to solve this family of problems. <br><br> Successor features (SF) separate environmental dynamics from rewards, while generalized policy improvement (GPI) considers multiple policies. Together, they facilitate cross-task information exchange in the RL framework [2]. <br><br> We can consider a environment with different tasks when only the reward function changes. If we try to combine SF with the occupancy MDP framework, we may be able to transfer learning among different tasks in Dec-POMDP problems. <br><br> This project will focus on the simulations aspects of this ideas, trying to get insight if this is possible by analysing the results of numerical experiments of algorithms that combine the tools previously described. </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/projects/four_room_ma.gif" class="img-fluid rounded z-depth-1 mb-3" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/projects/corner_weights.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption mt-2"> In the top figure, two agents must choose the path that gives more return depending on rewards given for collect triangles of different colors. In the bottom figure, a schematic representation of the algorithm that suggests how to add new policies to the set of optimum policies by solving different tasks. </div> </div> </div> <p><br></p> <h2 style="color: #00ab37;">Goal:</h2> <p>Suggest and run simulations of a new cooperative multi-agent reinforcement learning algorithm with a joint reward signal that combines the ideas of VDN, successor features, and the strategy to find a set of optimum policies as described in [4].</p> <p><br></p> <h2 style="color: #00ab37;">Preliminary work:</h2> <p>[1] proposes an algorithm to solve cooperative multi-agent reinforcement learning problems. [2] investigates how to do transfer learning using SF and GPI, and [4]’s work focuses on the algorithm to find the set of policies to deliver the optimum solution when using SF and GPI.</p> <p><br></p> <h2 style="color: #00ab37;">Tasks:</h2> <p>This project can include</p> <li>Read the literature on SF, GPI, MARL, and multi-objective environments.</li> <li>Choose a multi-agent environment and run some simulations using VDN.</li> <li>Slightly modify a multi-agent environment to treat it as a multi-objective problem.</li> <li>Propose an algorithm that combines VDN, SF, and the strategy used in [4] to identify the set of optimal policies.</li> <li>Run simulations with the proposed algorithm and assess the results.</li> <p><br> The final tasks will be discussed with the supervisor. Please feel free to get in contact.</p> <p><br></p> <h3 style="color: #00ab37;">References</h3> <li>[1] Barreto, André, et al. <a href="https://www.davidsilver.uk/wp-content/uploads/2020/09/Fast-reinforcement.pdf" rel="external nofollow noopener" target="_blank">Fast reinforcement learning with generalized policy updates.</a> <i>Proceedings of the National Academy of Sciences 117.48</i>, (2020): 30079-30087. Click <a href="https://www.youtube.com/watch?v=6_7vE08acVM" rel="external nofollow noopener" target="_blank">here</a> for a video presentation, and <a href="https://www.deepmind.com/blog/fast-reinforcement-learning-through-the-composition-of-behaviours" rel="external nofollow noopener" target="_blank">here</a> for the Google DeepMind blog post about the topic.</li> <li>[3] Dibangoye, Jilles Steeve, et al. <a href="http://proceedings.mlr.press/v139/gupta21a" rel="external nofollow noopener" target="_blank">Optimally solving Dec-POMDPs as continuous-state MDPs. </a> <i>Journal of Artificial Intelligence Research 55 (443-497)</i>, 2016.</li> <li>[5] Barreto, André, et al. <a href="https://proceedings.neurips.cc/paper/2017/hash/350db081a661525235354dd3e19b8c05-Abstract.html" rel="external nofollow noopener" target="_blank">Successor features for transfer in reinforcement learning.</a> <i>Advances in neural information processing systems 30</i>, 2017.</li> <li>[6] Alegre, Lucas N., et al. <a href="https://mo-gymnasium.farama.org/" rel="external nofollow noopener" target="_blank">MO-Gymnasium (Software) </a> <i>Multi-Objective Gymnasium type environment</i>, 2022.</li> <p><br></p> <h4 style="color: #00ab37;">Supervision</h4> <p>Supervisor: <a href="https://www.rug.nl/staff/r.f.cunha/?lang=en" rel="external nofollow noopener" target="_blank">Rafael Fernandes Cunha</a><br> Room: 5161.0438 (Bernoulliborg)<br> Email: r.f.cunha@rug.nl</p> <p>Dibangoye, Jilles Steeve, et al. “Optimally solving Dec-POMDPs as continuous-state MDPs.” Journal of Artificial Intelligence Research 55 (2016): 443-497.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Rafael Fernandes Cunha. Last updated: August 28, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>