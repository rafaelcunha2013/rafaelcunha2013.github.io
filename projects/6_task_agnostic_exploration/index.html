<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Task-agnostic exploration for sparse reward environments | Rafael Fernandes Cunha</title> <meta name="author" content="Rafael Fernandes Cunha"> <meta name="description" content="Combine task-agnostic exploration techniques with RL algorithms to solve sparse reward environments"> <meta name="keywords" content="reinforcement learning, multi-agent systems, control theory, machine learning, multi-agent reinforcement learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rafaelcunha2013.github.io/projects/6_task_agnostic_exploration/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">RafaelÂ </span>Fernandes Cunha</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Task-agnostic exploration for sparse reward environments</h1> <p class="post-description">Combine task-agnostic exploration techniques with RL algorithms to solve sparse reward environments</p> </header> <article> <style>h7:after{content:"\A";white-space:pre}</style> <h6 style="color: #00ab37;display: inline">Course of study:</h6> <h7 style="display: inline;">Artificial Intelligence</h7> <h6 style="color: #00ab37;display: inline">Kind of thesis:</h6> <h7 style="display: inline;">Theoretical analysis and Numerical Simulation</h7> <h6 style="color: #00ab37; display: inline">Programming languages:</h6> <h7 style="display: inline;">Python</h7> <h6 style="color: #00ab37; display: inline">Keywords:</h6> <h7 style="display: inline;">Reinforcement Learning, Transfer Learning, Sparse Rewards, Exploration</h7> <p><br></p> <h2 style="color: #00ab37;">Problem:</h2> <div class="row"> <div class="col-sm-8 mt-3 mt-md-0" style="text-align: justify;"> Exploration is a fundamental aspect of reinforcement learning (RL). Its effectiveness plays a crucial role in the performance of RL algorithms, particularly when confronted with sparse extrinsic rewards. In reinforcement learning (RL), exploration is generally divided into two settings. The first, task-driven exploration, involves a well-defined reward where the agent aims to explore to maximize long-term rewards. However, in real-life scenarios, external rewards are often sparse or completely unknown [1]. The second, task-agnostic exploration, occurs when an agent must explore a new environment without the guidance of any external reward <br><br> [1] describes a setup where the agent first learns to explore multiple environments in a task-agnostic manner without any extrinsic goal. Subsequently, the agent efficiently transfers this learned exploration policy to more effectively explore new environments while solving tasks. The researchers introduced the Change-Based Exploration Transfer (C-BET) algorithm, which merges exploration with both extrinsic and intrinsic goals. The C-BET algorithm can be integrated into various RL algorithms. In the paper, IMPALA was the chosen method <br><br> This project will focus on analyzing the C-BET algorithm and conducting simulations to understand its performance with both value-based and policy-based RL algorithms. What impacts do changes in hyperparameters have on the agent's performance? </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/projects/mini_grid_sparse_reward.png" class="img-fluid rounded z-depth-1 mb-3" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption mt-2"> Change-Based Exploration Transfer (C-BET) trains task-agnostic exploration agents that transfer to new environments. Here the agent learns that keys are interesting, as they allow further interaction with the environment (opening doors). Later, when tasked with reaching a box behind a door, the agent starts by picking up the key. (Extracted from [1]) </div> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/projects/exploration_transfer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption mt-2"> On the left, C-BET pre-training scheme. The agent interacts with environments and learns using intrinsic rewards computed from state and change counts. On the right, C-BET transfer, the pre-trained exploration policy is fixed and guides task-specific policy learning in new environments (Extracted from [1]). </div> </div> </div> <h2 style="color: #00ab37;">Goal:</h2> <p>Integrate the C-BET algorithm with both a value-based and a policy-based RL algorithm within the mini-grid environment [2], and then analyze the outcomes. How do these results compare to the IMPALA algorithm [3] utilized in the paper?</p> <p><br></p> <h2 style="color: #00ab37;">Preliminary work:</h2> <p>[1] introduces an algorithm designed to address sparse reward environments by employing task-agnostic exploration techniques. Simulations were conducted in the mini-grid environment, utilizing a blend of the C-BET and IMPALA algorithms. The study also presented a benchmark and metrics for evaluating task-agnostic exploration strategies.</p> <p><br></p> <h2 style="color: #00ab37;">Tasks:</h2> <p>This project can include</p> <li>Review literature addressing solutions for sparse reward environments.</li> <li>Grasp the intricacies of the C-BET algorithm and its relation to task-agnostic exploration.</li> <li>Select both a value-based and a policy-based RL algorithm for integration with the C-BET approach.</li> <li>Execute simulations using the proposed algorithm and evaluate the outcomes.</li> <p><br> The final tasks will be discussed with the supervisor. Please feel free to get in contact.</p> <p><br></p> <h3 style="color: #00ab37;">References</h3> <li>[1] Parisi, Simone, et al. <a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/abe8e03e3ac71c2ec3bfb0de042638d8-Abstract.html" rel="external nofollow noopener" target="_blank">Interesting object, curious agent: Learning task-agnostic exploration.</a> <i>Advances in Neural Information Processing Systems 34</i>, (2021): 20516-20530.</li> <li>[2] Chevalier-Boisvert, Maxime, et al. <a href="https://arxiv.org/abs/2306.13831" rel="external nofollow noopener" target="_blank">Minigrid &amp; Miniworld: Modular &amp; Customizable Reinforcement Learning Environments for Goal-Oriented Tasks</a> <i>arXiv preprint arXiv:2306.13831</i>, 2023. Check also <a href="https://minigrid.farama.org/" rel="external nofollow noopener" target="_blank">this</a> page for the documentation.</li> <li>[3] Espeholt, Lasse, et al. <a href="http://proceedings.mlr.press/v80/espeholt18a.html" rel="external nofollow noopener" target="_blank">Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.</a> <i>International conference on machine learning. PMLR</i>, 2018.</li> <li>[4] Wan, Shanchuan, et al. <a href="https://arxiv.org/abs/2304.10770" rel="external nofollow noopener" target="_blank">DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards.</a> <i>arXiv preprint arXiv:2304.10770 (IJCAI)</i>, 2023.</li> <p><br></p> <h4 style="color: #00ab37;">Supervision</h4> <p>Supervisor: <a href="https://www.rug.nl/staff/r.f.cunha/?lang=en" rel="external nofollow noopener" target="_blank">Rafael Fernandes Cunha</a><br> Room: 5161.0438 (Bernoulliborg)<br> Email: r.f.cunha@rug.nl</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2023 Rafael Fernandes Cunha. Last updated: August 28, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>