<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Faithfulness of explanations in deep reinforcement learning | Rafael Fernandes Cunha</title> <meta name="author" content="Rafael Fernandes Cunha"> <meta name="description" content="Are input attribution methods applied to deep reinforcement learning agents faithful to the policy learned?"> <meta name="keywords" content="reinforcement learning, multi-agent systems, control theory, machine learning, multi-agent reinforcement learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rafaelcunha2013.github.io/projects/8_xrl/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Rafael </span>Fernandes Cunha</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Faithfulness of explanations in deep reinforcement learning</h1> <p class="post-description">Are input attribution methods applied to deep reinforcement learning agents faithful to the policy learned?</p> </header> <article> <style>h7:after{content:"\A";white-space:pre}</style> <h6 style="color: #00ab37;display: inline">Course of study:</h6> <h7 style="display: inline;">Artificial Intelligence</h7> <h6 style="color: #00ab37;display: inline">Kind of thesis:</h6> <h7 style="display: inline;">Theoretical analysis and Numerical Simulation</h7> <h6 style="color: #00ab37; display: inline">Programming languages:</h6> <h7 style="display: inline;">Python</h7> <h6 style="color: #00ab37; display: inline">Keywords:</h6> <h7 style="display: inline;">Explanable Reinforcement Learning (XRL), Explanable AI, Input Attribution (IA) </h7> <p><br></p> <h2 style="color: #00ab37;">Problem:</h2> <div class="row"> <div class="col-sm-8 mt-3 mt-md-0" style="text-align: justify;"> Neural Networks (NNs) are often cited as black-boxes, i.e., models which are not interpretable by human cognition and whose function learnt from the data is not directly accessible. For this reason, several techniques for trying to probe into one or more aspects of the inner functioning of NNs have been proposed, and this field has been dubbed <i>Explainable AI</i> (XAI). Input Attribution (IA) methods are tools to approximate which parts of an input are important in determining the model prediction. <br><br> NNs can also be employed as controllers of Reinforcement Learning (RL)-based agents, whereas the model is used to learn the best policy for navigating an environment in a given state. In this sense, IA methods, or any other explainability technique, can be a great tool for getting intuitions about the reasons why an agent executes a specific action. The field that looks for explainability in RL agent behaviors is called explainable RL (XRL). The goal of XRL is to elucidate the decision-making process of learning agents in sequential decision-making settings. <br><br> One of the most pressing challenges of XAI is connected to the assessment of the <i>quality</i> of the explanations provided: in fact, many XAI tools are mere approximations of the underlying decision process operated by the NN, and the explanations can be widely inaccurate in that regard. A straightforward way of evaluating the explanations is to consider the <i>faithfulness</i> of the explanation: for instance, we could ask ourselves whether the input parts identified via IA are actually important for the NN—by perturbing or masking these areas, one reasonably expects the action of the underlying RL agent to change. </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/projects/xrl_taxonomy.png" class="img-fluid rounded z-depth-1 mb-3" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/projects/explainable-ai.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption mt-2"> In the top figure, there's the XRL taxonomy and its relationship to the RL process [4]. In the bottom figure, there are some reasons to study explainable AI. </div> </div> </div> <p><br></p> <h2 style="color: #00ab37;">Goal:</h2> <p>Are input attribution methods applied to deep reinforcement learning agents faithful to the policy learned?</p> <p><br></p> <h2 style="color: #00ab37;">Preliminary work:</h2> <p>In [1], you can find an introductory course on XAI. [2] offers a comprehensive introduction to the evaluation of XAI techniques. [3] provides an example where input attribution methods exhibit low faithfulness, and [4] presents a literature review of some XRL methods.</p> <p><br></p> <h2 style="color: #00ab37;">Tasks:</h2> <p>This project can include</p> <li>Literature review on XAI and XRL.</li> <li>Literature review on assessing quality of XAI tools.</li> <li>Run simulations in a chosen atari gym environment. </li> <li>Analyze results.</li> <p><br> The final tasks will be discussed with the supervisor. Please feel free to get in contact.</p> <p><br></p> <h3 style="color: #00ab37;">References</h3> <li>[1] <a href="https://hcixaitutorial.github.io/" rel="external nofollow noopener" target="_blank"> Course on XAI. </a> </li> <li>[2] Nauta, Meike, et al. <a href="https://arxiv.org/abs/2201.08164" rel="external nofollow noopener" target="_blank">From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai.</a> <i>ACM Computing Surveys 55.13s: 1-42</i>, 2023.</li> <li>[3] Arrighi, Leonardo, et al. <a href="https://drive.google.com/file/d/1WpfTWjSV6uS576-uF27NHzZBnWnWiN-8/view" rel="external nofollow noopener" target="_blank">Explainable Automated Anomaly Recognition in Failure Analysis: is Deep Learning Doing it Correctly?.</a> <i> XAI conference (2023). Under publication.</i>, 2023.</li> <li>[4] Milani, Stephanie, et al. <a href="https://arxiv.org/abs/2202.08434" rel="external nofollow noopener" target="_blank">A survey of explainable reinforcement learning.</a> <i>arXiv preprint arXiv:2202.08434</i>, 2022.</li> <li>[5] Milani, Stephanie, et al. <a href="https://gymnasium.farama.org/environments/atari/complete_list/" rel="external nofollow noopener" target="_blank">A complete list of atari gym environments.</a> </li> <p><br></p> <h4 style="color: #00ab37;">Supervision</h4> <p>Supervisor: <a href="https://www.rug.nl/staff/r.f.cunha/?lang=en" rel="external nofollow noopener" target="_blank">Rafael Fernandes Cunha</a><br> Room: 5161.0438 (Bernoulliborg)<br> Email: r.f.cunha@rug.nl</p> <p>Supervisor: <a href="https://www.rug.nl/staff/m.zullich/?lang=en" rel="external nofollow noopener" target="_blank">Marco Zullich</a><br> Room: 5161.0438 (Bernoulliborg)<br> Email: m.zullich@rug.nl</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Rafael Fernandes Cunha. Last updated: September 02, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>